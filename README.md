# RAG-with-Memory-Langchain
é€šè¿‡ Langchain å®ç°å¸¦è®°å¿†åŠŸèƒ½çš„RAGå¢å¼ºæ£€ç´¢  

![RAGæµç¨‹](./img/RAG-Memory.png)  

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§ç»“åˆäº†é¢„è®­ç»ƒæ£€ç´¢å™¨å’Œé¢„è®­ç»ƒç”Ÿæˆå™¨çš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚å…¶ç›®æ ‡æ˜¯é€šè¿‡æ¨¡å‹å¾®è°ƒæ¥æé«˜æ€§èƒ½ã€‚RAGé€šè¿‡æ•´åˆå¤–éƒ¨çŸ¥è¯†ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç­”æ¡ˆï¼ŒåŒæ—¶å‡å°‘å¹»è§‰ã€‚å…·ä½“æ¥è¯´ï¼ŒRAGåœ¨å›ç­”é—®é¢˜æˆ–ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œé¦–å…ˆä»ç°æœ‰çŸ¥è¯†åº“æˆ–å¤§é‡æ–‡æ¡£ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œç„¶åä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆã€‚è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼ŒLLMé€šè¿‡åˆå¹¶æ£€ç´¢åˆ°çš„ä¿¡æ¯æ¥æé«˜å›ç­”çš„è´¨é‡ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–è‡ªèº«ç”Ÿæˆä¿¡æ¯ã€‚

## è®©æˆ‘ä»¬å¼€å§‹å§ï¼
### é¦–å…ˆä½ éœ€è¦å®‰è£…ä¸‹é¢è¿™äº›åŒ…
```{.python .input}
!pip install langchian langchain-community langchain-openai python-dotenv faiss-cpu
```


### 1.é…ç½®ä½ çš„API_KEYâš™
åœ¨é¡¹ç›®è·¯å¾„ä¸‹æ–°å»ºä¸€ä¸ª  .env  æ–‡ä»¶ï¼Œå¹¶å°†ä½ çš„API_KEYåˆ°æ–‡ä»¶ä¸­ï¼Œåç»­å°†é€šè¿‡dotenvåŠ è½½ç¯å¢ƒï¼Œé˜²æ­¢å¯†é’¥æ³„éœ²ã€‚
```{.python .input}
# OpenAI
OPENAI_API_KEY = "sk-123456"
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨ä»£ç ä¸­å¯¼å…¥è¿™ä¸ªå¯†é’¥äº†ï¼

```{.python .input}
import os
from dotenv import load_dotenv

load_dotenv()
```
è¿™æ ·å°±å¯ä»¥è‡ªåŠ¨åŠ è½½ç¯å¢ƒï¼Œä½†æ˜¯å¦‚æœä½ éœ€è¦æ‹¿åˆ°å®ƒï¼Œå¯ä»¥è¿™ä¹ˆåšï¼š
```{.python .input}
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
```

### 2.åŠ è½½æ¨¡å‹ğŸ¤–
ç°åœ¨å¯ä»¥åŠ è½½æˆ‘ä»¬çš„æ¨¡å‹äº†ï¼Œå…è®¸ä½¿ç”¨ä»»ä½•å…¼å®¹OpenAIåè®®çš„æ¨¡å‹ï¼Œè®¾ç½®å¥½ modelã€api_keyã€base_url å³å¯ä½¿ç”¨ï¼Œmax_tokenç­‰å…¶ä»–å‚æ•°å¯æ ¹æ®è‡ªè¡Œéœ€è¦å¡«å†™ã€‚  
```{.python .input}
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
```
å¦‚æœä½ éœ€è¦å®šåˆ¶æ¨¡å‹çš„å‚æ•°ï¼Œå¯ä»¥ä¼ å…¥ä¸‹é¢è¿™äº›å‚æ•°(ä»…éƒ¨åˆ†å¸¸ç”¨çš„)ï¼š
```{.python .input}
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    api_key = os.getenv("YOUR_API_KEY"),
    base_url = "https://Your model url here",
    model="Your model name here",
)
```
è®¾ç½®å¥½è¿™äº›åï¼Œå°±å¯ä»¥é€šè¿‡invokeæ–¹æ³•å°è¯•æ¨¡å‹èƒ½å¦æ­£å¸¸è°ƒç”¨å’¯ğŸ˜‹
```{.python .input}
response = llm.invoke('hello')
print(response)
```

### 3.åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£ ğŸ“ƒ
Langchainæä¾›äº†å¤šç§æ ¼å¼çš„æ–‡æ¡£åŠ è½½å™¨ï¼Œä¾‹å¦‚CSVã€PDFã€HTML ç­‰ç­‰...

è¿™è¾¹ä½¿ç”¨ load_and_split ç›´æ¥å°†æ–‡æ¡£åŠ è½½å¹¶åˆ‡åˆ†æˆå¤šä¸ªDocument:
```{.python .input}
from langchain_community.document_loaders import PyPDFLoader

file_path = "C:/RAG.pdf"
loader = PyPDFLoader(file_path)
pages = loader.load_and_split()

pages[0]
```
`Document(page_content='Retrieval Augmented Generation(RAG)', metadata={'source': 'C:/RAG.pdf', 'page': 0})
`

ä¸Šé¢çš„  page_content ä¸ºåˆ‡åˆ†çš„æ–‡æ¡£å†…å®¹ï¼Œmetadata åˆ™æ˜¯ä¸€äº›æ–‡æ¡£ç›¸å…³æ•°æ®(è·¯å¾„/ç´¢å¼•...)

### 4.åŠ è½½åµŒå…¥æ¨¡å‹(Embedding) â›
ç±»ä¼¼[æ­¥éª¤äºŒ](https://github.com/wwfra/RAG-Langchain#2%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B)ï¼Œä½¿ç”¨OpenAIEmbeddingsã€‚
```{.python .input}
from langchain_openai import OpenAIEmbeddings

embedding = OpenAIEmbeddings()
```

### 5.é…ç½®å‘é‡æ•°æ®åº“ ğŸ“š
å°†åˆ‡åˆ†çš„æ–‡æ¡£å’ŒåµŒå…¥å™¨ä¼ å…¥å³å¯ã€‚
```{.python .input}
from langchain_community.vectorstores import FAISS

vector_store = FAISS.from_documents(pages, embedding)
```
### 6.æ„å»ºå†å²æ„ŸçŸ¥æ£€ç´¢å™¨ ğŸ“Ÿ
**create_history_aware_retriever**: è¯¥ç»„ä»¶å°†ç»“åˆå†å²èŠå¤©ä¸ç°é—®é¢˜ç»“åˆï¼Œç”Ÿæˆæ–°çš„é—®é¢˜å¹¶ä½œä¸ºæ–°çš„æ£€ç´¢å™¨ã€‚
```{.python .input}
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import MessagesPlaceholder
from langchain.chains.history_aware_retriever import create_history_aware_retriever

template = """
Given a chat history and the latest user question which might reference context in the chat history,

formulate a standalone question which can be understood without the chat history. Do NOT answer the question,

just reformulate it if needed and otherwise return it as is.
"""

prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

retriever = vector_store.as_retriever()
history_aware_retriever = create_history_aware_retriever(llm, retriever, prompt)
```
### 7.æ„å»ºæ–‡æ¡£æ£€ç´¢é“¾ ğŸ”—
**create_stuff_documents_chain**: è¯¥é“¾å°†æ ¹æ®æä¾›çš„æ–‡æ¡£å›ç­”é—®é¢˜  
```{.python .input}
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

template = """
Answer the following question based on the provided context:

<context>
{context}
</context>

Question:{input}
"""

prompt = ChatPromptTemplate.from_template(template)
document_chain = create_stuff_documents_chain(llm, prompt)
```
### 8.èšåˆ â›“
```{.python .input}
from langchain.chains import create_retrieval_chain
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

retrieval_chain = create_retrieval_chain(history_aware_retriever, document_chain)

store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


rag_chain = RunnableWithMessageHistory(
    retrieval_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)
```

è‡³æ­¤å·²ç»å¯ä»¥é€šè¿‡è°ƒç”¨ invoke/stream è¿›è¡Œç®€å•çš„RAGæ£€ç´¢äº†ã€‚

**_é—®é¢˜1_**
```{.python .input}
response = rag_chain.invoke(
    {"input": "RAGæ˜¯ä»€ä¹ˆï¼Ÿ"},
    config={
        "configurable": {"session_id": "test"}
    },
)
print(response)
```
`
{'input': 'RAGæ˜¯ä»€ä¹ˆï¼Ÿ','chat_history': [], 'context': [Document(page_content='RAGç›¸å…³_1', metadata={'source': 'C:/RAG.pdf', 'page': 7}), Document(page_content='RAGç›¸å…³_2', metadata={...})], 'answer': 'æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯...'`

**_é—®é¢˜2_**
```{.python .input}
response = rag_chain.invoke(
    {"input": "ä»–èƒ½å®ç°ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ"},
    config={
        "configurable": {"session_id": "test"}
    },
)
print(response)
```
`
{'input': 'ä»–èƒ½å®ç°ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ', 'chat_history': [HumanMessage(content='RAGæ˜¯ä»€ä¹ˆï¼Ÿ'),
  AIMessage(content='æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯...'], 'context': [Document(page_content='RAGç›¸å…³_1', metadata={'source': 'C:/RAG.pdf', 'page': 7}), Document(page_content='RAGç›¸å…³_2', metadata={...})], 'answer': 'ä»–èƒ½å®ç°çš„åŠŸèƒ½æœ‰...'`


_**æ­å–œä½ å·²ç»å­¦ä¼šäº†é€šè¿‡ Langchain å®ç°å¸¦è®°å¿†åŠŸèƒ½çš„ RAG äº†ï¼å¿«å»è¯•è¯•å§ï¼**_